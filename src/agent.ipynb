{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mine_sweeper_env\n",
    "import torch\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "class ReplayMemory:\n",
    "    def __init__(self, max_size, batch_size) -> None:\n",
    "        self.max_size = max_size\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        self.memory = list()\n",
    "        self.pointer = 0\n",
    "        \n",
    "    def log_memory(self, memory):\n",
    "        if len(self.memory < self.max_size):\n",
    "            self.memory.append(memory)\n",
    "        else:\n",
    "            self.memory[self.pointer] = memory\n",
    "            self.pointer += 1\n",
    "            if self.pointer >= len(self.memory):\n",
    "                self.pointer = 0\n",
    "    \n",
    "    def get_sample(self, size:int=0):\n",
    "        if size == 0:\n",
    "            size = self.batch_size\n",
    "        return random.sample(self.memory, size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "class Actor(nn.Module):\n",
    "    def __init__(self, board_width, board_height) -> None:\n",
    "        super().__init__()\n",
    "        input_size = board_width * board_height\n",
    "        output_size = 2\n",
    "        hidden_size = int((input_size + output_size)/2)\n",
    "        \n",
    "        self.linear1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.linear2 = nn.Linear(hidden_size, output_size)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, input:torch.Tensor) -> torch.Tensor:\n",
    "        output = self.relu(self.linear1(input))\n",
    "        output = self.sigmoid(self.linear2(output))\n",
    "        return output\n",
    "\n",
    "\n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, board_width, board_height) -> None:\n",
    "        super().__init__()\n",
    "        input_size = board_width * board_height + 2\n",
    "        output_size = 1\n",
    "        hidden_size = int((input_size + output_size)/2)\n",
    "\n",
    "        self.linear1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.linear2 = nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "    def forward(self, states:torch.Tensor, actions:torch.Tensor) -> torch.Tensor:\n",
    "        output = torch.concat([states, actions], dim=1)\n",
    "        output = self.relu(self.linear1(output))\n",
    "        output = self.linear2(output)\n",
    "        return output\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import mlflow.pytorch\n",
    "import os\n",
    "\n",
    "os.environ['GIT_PYTHON_REFRESH'] = 'quiet'\n",
    "\n",
    "def train():\n",
    "    exp = mlflow.set_experiment(\"Actor_critic\")\n",
    "    mlflow.start_run(experiment_id=exp.experiment_id)\n",
    "\n",
    "    # Start of Hyperparameters\n",
    "    BOARD_SIZE = 10\n",
    "    NUMBER_OF_MINTES = 15\n",
    "\n",
    "    LEARNING_RATE = 1E-3\n",
    "    BETAS = (0.9, 0.99)\n",
    "    MAX_ITERATIONS_FOR_CONVERGENCE = 1000\n",
    "\n",
    "    MAX_REPLAY_MEMORY_SIZE = 10000\n",
    "    BATCH_SIZE = 512\n",
    "    MIN_REPLAY_MEMORY_SIZE_TO_START_TRAINING = 1000\n",
    "    UPDATE_TARGET_NET_PER_STEPS = 100\n",
    "\n",
    "    MAX_EPISODES = 10000\n",
    "\n",
    "    TD_LAMBDA = 5\n",
    "    DISCOUNT_FACTOR = 0.99\n",
    "    # End of hyperparameters\n",
    "\n",
    "    # Logging hyperparameters using mlflow\n",
    "    mlflow.log_param('BOARD_SIZE', BOARD_SIZE)\n",
    "    mlflow.log_param('NUMBER_OF_MINTES', NUMBER_OF_MINTES)\n",
    "    mlflow.log_param('LEARNING_RATE', LEARNING_RATE)\n",
    "    mlflow.log_param('BETAS', BETAS)\n",
    "    mlflow.log_param('MAX_ITERATIONS_FOR_CONVERGENCE', MAX_ITERATIONS_FOR_CONVERGENCE)\n",
    "    mlflow.log_param('MAX_REPLAY_MEMORY_SIZE', MAX_REPLAY_MEMORY_SIZE)\n",
    "    mlflow.log_param('BATCH_SIZE', BATCH_SIZE)\n",
    "    mlflow.log_param('MIN_REPLAY_MEMORY_SIZE_TO_START_TRAINING', MIN_REPLAY_MEMORY_SIZE_TO_START_TRAINING)\n",
    "    mlflow.log_param('UPDATE_TARGET_NET_PER_STEPS', UPDATE_TARGET_NET_PER_STEPS)\n",
    "    mlflow.log_param('MAX_EPISODES', MAX_EPISODES)\n",
    "    mlflow.log_param('TD_LAMBDA', TD_LAMBDA)\n",
    "    mlflow.log_param('DISCOUNT_FACTOR', DISCOUNT_FACTOR)\n",
    "    # End of logging hyperparameters\n",
    "    \n",
    "    # Initializing networks\n",
    "    actor_net = Actor(board_height=BOARD_SIZE, board_width=BOARD_SIZE)\n",
    "    critic_net = Critic(board_height=BOARD_SIZE, board_width=BOARD_SIZE)\n",
    "    target_actor_net = Actor(board_height=BOARD_SIZE, board_width=BOARD_SIZE)\n",
    "    target_critic_net = Critic(board_height=BOARD_SIZE, board_width=BOARD_SIZE)\n",
    "    target_actor_net.load_state_dict(actor_net.state_dict())\n",
    "    target_critic_net.load_state_dict(critic_net.state_dict())\n",
    "    target_actor_net.eval()\n",
    "    target_critic_net.eval()\n",
    "\n",
    "    # Loss function and optimizer\n",
    "    loss_function = torch.nn.functional.mse_loss \n",
    "    optimizer_actor = torch.optim.Adam(params=actor_net.parameters(), lr=LEARNING_RATE, betas=BETAS)\n",
    "    optimizer_critic = torch.optim.Adam(params=critic_net.parameters(), lr=LEARNING_RATE, betas=BETAS)\n",
    "\n",
    "    # Preparing environment, and replay memory\n",
    "    env = mine_sweeper_env.MinesweeperEnv(board_size=BOARD_SIZE, num_mines=NUMBER_OF_MINTES)\n",
    "    replay_memory = ReplayMemory(max_size=MAX_REPLAY_MEMORY_SIZE, batch_size=BATCH_SIZE)\n",
    "\n",
    "    total_step_count = 0\n",
    "    for episode in range(MAX_EPISODES):\n",
    "        is_episode_done = False\n",
    "        total_reward = 0\n",
    "\n",
    "        steps_in_this_epoch = 0\n",
    "        while is_episode_done is False:\n",
    "            # Go through the environment for labmda number of steps\n",
    "            states_list = list()\n",
    "            actions_list = list()\n",
    "            rewards_list = list()\n",
    "\n",
    "            td_lambda_count = 0\n",
    "            while td_lambda_count < TD_LAMBDA and not(is_episode_done):\n",
    "                # Get an observation\n",
    "                observation = torch.tensor(env.my_board).flatten()\n",
    "                states_list.append(observation)\n",
    "\n",
    "                # Prediction of action using actor net\n",
    "                actor_net.eval()\n",
    "                action = actor_net(observation[None, :]).flatten()\n",
    "                actions_list.append(action)\n",
    "                \n",
    "                # Add noise to action here for exploration purpose\n",
    "\n",
    "                # Performing the action in the environment\n",
    "                final_action = action * BOARD_SIZE\n",
    "                new_state, reward, is_episode_done, _ = env.step(final_action.numpy())\n",
    "                \n",
    "                total_reward += reward\n",
    "                rewards_list.append(reward)\n",
    "\n",
    "                td_lambda_count += 1\n",
    "            \n",
    "            # Saving the observations obtained\n",
    "            target_q_values = [0]*len(states_list)\n",
    "            if is_episode_done:\n",
    "                last_target = 0\n",
    "            else:\n",
    "                output_of_actor = target_actor_net(torch.tensor(new_state)[None, :]).flatten()\n",
    "                last_target = target_critic_net(torch.tensor(new_state)[None, :], output_of_actor).item()\n",
    "            target_q_values [-1] = rewards_list[-1] + DISCOUNT_FACTOR * last_target\n",
    "\n",
    "            # Calculating all target q values\n",
    "            for i in range(len(target_q_values)-2, -1, -1):\n",
    "                target_q_values[i] = rewards_list[i] + DISCOUNT_FACTOR * target_q_values[i+1]\n",
    "\n",
    "            for item in zip(states_list, actions_list, target_q_values):\n",
    "                replay_memory.log_memory(item)\n",
    "\n",
    "            # Training the nets\n",
    "            if len(replay_memory.memory) > MIN_REPLAY_MEMORY_SIZE_TO_START_TRAINING:\n",
    "                # Training critic net first\n",
    "                critic_net.train()\n",
    "                prev_loss = 0\n",
    "                for i in range(MAX_ITERATIONS_FOR_CONVERGENCE):\n",
    "                    optimizer_critic.zero_grad()\n",
    "                    batch = replay_memory.get_sample()\n",
    "                    \n",
    "                    states_list = [item[0] for item in batch]\n",
    "                    actions_list = [item[1] for item in batch]\n",
    "                    target_q_values = [item[2] for item in batch]\n",
    "                    \n",
    "                    predicted_q_values = critic_net(states_list)\n",
    "                    target_q_values = torch.tensor(target_q_values)\n",
    "\n",
    "                    loss = loss_function(predicted_q_values.flatten(), target_q_values)\n",
    "                    loss.backward()\n",
    "                    optimizer_critic.step()\n",
    "\n",
    "                    # Check for convergence for an early exit\n",
    "                    change_in_loss = abs(prev_loss - loss.item()) / loss.item()\n",
    "                    if change_in_loss < 0.001:\n",
    "                        print('Exiting due to early convergence!!! at step', i, '-> prev loss:', prev_loss, 'current loss:', loss.item())\n",
    "                        break\n",
    "                    else:\n",
    "                        prev_loss = loss.item()\n",
    "                        print('Steps in training', i)\n",
    "                \n",
    "                critic_net.eval()\n",
    "\n",
    "                # Now, training actor net to maximize Q value.\n",
    "                prev_loss = 0\n",
    "                actor_net.train()\n",
    "                for i in range(MAX_ITERATIONS_FOR_CONVERGENCE):\n",
    "                    optimizer_actor.zero_grad()\n",
    "                    batch = replay_memory.get_sample()\n",
    "                    \n",
    "                    states_list = [item[0] for item in batch]\n",
    "                    states_list = torch.stack(states_list)\n",
    "\n",
    "                    output_of_actor = actor_net(states_list)\n",
    "                    final_q_values = critic_net(states_list, output_of_actor)\n",
    "                    loss = -1 * torch.sum(final_q_values)\n",
    "                    loss.backward()\n",
    "                    optimizer_actor.step()\n",
    "\n",
    "                    # Check for convergence for an early exit\n",
    "                    change_in_loss = abs(prev_loss - loss.item()) / loss.item()\n",
    "                    if change_in_loss < 0.001:\n",
    "                        print('Exiting due to early convergence!!! at step', i, '-> prev loss:', prev_loss, 'current loss:', loss.item())\n",
    "                        break\n",
    "                    else:\n",
    "                        prev_loss = loss.item()\n",
    "                        print('Steps in training', i)\n",
    "                \n",
    "                actor_net.eval()\n",
    "            \n",
    "            steps_in_this_epoch += 1\n",
    "            total_step_count += 1\n",
    "            if total_step_count % UPDATE_TARGET_NET_PER_STEPS == 0:\n",
    "                target_actor_net.load_state_dict(actor_net.state_dict())\n",
    "                target_critic_net.load_state_dict(critic_net.state_dict())\n",
    "            \n",
    "        # Saving metrics and models after every epoch\n",
    "        mlflow.log_metric('total_reward', total_reward, step=episode)\n",
    "\n",
    "        # evaluate() # Write an evaluation function then log metrics with step as episode number\n",
    "\n",
    "        mlflow.pytorch.log_model(actor_net, artifact_path='Actor_net_after_episode_'+str(episode))\n",
    "        mlflow.pytorch.log_model(critic_net, artifact_path='Critic_net_after_episode_'+str(episode))\n",
    "\n",
    "        print(f\"Episode: {episode:5d}\" ,f\"Total reward: {total_reward:.2f}\")\n",
    "\n",
    "        # Resetting the environment\n",
    "        env.reset()\n",
    "        is_episode_done = False\n",
    "\n",
    "    mlflow.end_run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "Run with UUID 3cd7c8667e424a3f9b20d3ffe90625f9 is already active. To start a new run, first end the current run with mlflow.end_run(). To start a nested run, call start_run with nested=True",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mException\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [9], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m train()\n",
      "Cell \u001b[1;32mIn [8], line 9\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m()\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtrain\u001b[39m():\n\u001b[0;32m      8\u001b[0m     exp \u001b[39m=\u001b[39m mlflow\u001b[39m.\u001b[39mset_experiment(\u001b[39m\"\u001b[39m\u001b[39mActor_critic\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m----> 9\u001b[0m     mlflow\u001b[39m.\u001b[39;49mstart_run(experiment_id\u001b[39m=\u001b[39;49mexp\u001b[39m.\u001b[39;49mexperiment_id)\n\u001b[0;32m     11\u001b[0m     \u001b[39m# Start of Hyperparameters\u001b[39;00m\n\u001b[0;32m     12\u001b[0m     BOARD_SIZE \u001b[39m=\u001b[39m \u001b[39m10\u001b[39m\n",
      "File \u001b[1;32md:\\my projects\\mine_sweeper\\.conda\\lib\\site-packages\\mlflow\\tracking\\fluent.py:270\u001b[0m, in \u001b[0;36mstart_run\u001b[1;34m(run_id, experiment_id, run_name, nested, tags, description)\u001b[0m\n\u001b[0;32m    268\u001b[0m experiment_id \u001b[39m=\u001b[39m \u001b[39mstr\u001b[39m(experiment_id) \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(experiment_id, \u001b[39mint\u001b[39m) \u001b[39melse\u001b[39;00m experiment_id\n\u001b[0;32m    269\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(_active_run_stack) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m nested:\n\u001b[1;32m--> 270\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mException\u001b[39;00m(\n\u001b[0;32m    271\u001b[0m         (\n\u001b[0;32m    272\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mRun with UUID \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m is already active. To start a new run, first end the \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    273\u001b[0m             \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mcurrent run with mlflow.end_run(). To start a nested \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    274\u001b[0m             \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mrun, call start_run with nested=True\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    275\u001b[0m         )\u001b[39m.\u001b[39mformat(_active_run_stack[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39minfo\u001b[39m.\u001b[39mrun_id)\n\u001b[0;32m    276\u001b[0m     )\n\u001b[0;32m    277\u001b[0m client \u001b[39m=\u001b[39m MlflowClient()\n\u001b[0;32m    278\u001b[0m \u001b[39mif\u001b[39;00m run_id:\n",
      "\u001b[1;31mException\u001b[0m: Run with UUID 3cd7c8667e424a3f9b20d3ffe90625f9 is already active. To start a new run, first end the current run with mlflow.end_run(). To start a nested run, call start_run with nested=True"
     ]
    }
   ],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 (conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "fca02f1d858251dd70b7435fc4386a59378a2c1334d0d237599b4e55c0838495"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
